# ============================================================================
# PROXY SETTINGS
# ============================================================================
PROXY_PORT=3000
PROXY_ACCESS_TOKEN=your-static-proxy-token-here

# ============================================================================
# TARGET ENDPOINT
# ============================================================================
TARGET_ENDPOINT=https://your-llm-endpoint.com/v1
USE_PLACEHOLDER_MODE=true  # Set to false when connecting to real endpoint

# Remove temperature=0 from requests (some models like GPT-5 don't support it)
STRIP_ZERO_TEMPERATURE=true

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
# Define the models your endpoint supports (comma-separated list)
# These will be returned by the /v1/models endpoint
AVAILABLE_MODELS=gpt-4,gpt-4-turbo,gpt-4o,gpt-4o-mini,gpt-3.5-turbo

# Default model for placeholder responses
DEFAULT_MODEL=gpt-4

# Small/fast model for minor tasks
DEFAULT_SMALL_MODEL=gpt-3.5-turbo

# Maximum output tokens for responses
# IMPORTANT: This is injected into requests when client doesn't specify max_tokens
# OpenAI Codex CLI doesn't send max_tokens, causing responses to truncate!
# Set this to your backend's max output tokens (e.g., 32768, 128000, 200000)
# Common values:
#   - GPT-4: 4096 (default) or 8192 (turbo)
#   - GPT-4o: 16384
#   - Custom models: Check your backend's limits
MAX_TOKENS=32768

# ============================================================================
# MODEL MAPPING (Optional)
# ============================================================================
# Map specific incoming model names to your target models
# Format: source-model=target-model,source2=target2
# Example: MODEL_MAPPING=gpt-4-custom=your-model-v1,gpt-3.5-custom=your-model-v2
# MODEL_MAPPING=

# ============================================================================
# OAUTH AUTHENTICATION
# ============================================================================
OAUTH_TOKEN_ENDPOINT=https://auth.yourcompany.com/oauth/token
OAUTH_CLIENT_ID=your-client-id
OAUTH_CLIENT_SECRET=your-client-secret

# OAuth scope (optional - leave commented if not required by your OAuth server)
# If you get "invalid_scope" error, either comment this out or ask your admin for the correct scope
# OAUTH_SCOPE=api.read

OAUTH_REFRESH_BUFFER_MINUTES=5

# ============================================================================
# ALTERNATIVE: SIMPLE API KEY (if not using OAuth)
# ============================================================================
# TARGET_API_KEY=sk-your-api-key

# ============================================================================
# DEVELOPMENT MODE (bypasses OAuth and rbc_security)
# ============================================================================
# DEV_MODE=true

# ============================================================================
# CORPORATE NETWORK SETTINGS (for GPT Researcher web search)
# ============================================================================
# If behind a corporate proxy, set these for DuckDuckGo search to work:
# CORPORATE_HTTP_PROXY=http://proxy.company.com:8080
# CORPORATE_HTTPS_PROXY=http://proxy.company.com:8080

# If corporate proxy uses SSL inspection, you may need to disable SSL verification
# WARNING: Only use this if you trust your corporate network
# DISABLE_SSL_VERIFY=true

# ============================================================================
# GPT RESEARCHER SETTINGS (to avoid DuckDuckGo rate limits)
# ============================================================================
# DuckDuckGo has rate limits - reduce these if you get 202 errors
# Defaults: 3 results/query, 2 iterations, 2 subtopics (12 total searches)
# MAX_SEARCH_RESULTS_PER_QUERY=3
# MAX_ITERATIONS=2
# MAX_SUBTOPICS=2
